{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b423b3",
   "metadata": {},
   "source": [
    "## File for searching book recomendations\n",
    "Recommendations are not based on overall ratings, but potentially should be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb965e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/anaconda3/envs/prawn/lib/python3.7/site-packages/thefuzz/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import random\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import collections\n",
    "import string\n",
    "import io\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as gensim_api\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eb68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
    "# Stopwords to filter out. This list could be made more comprehensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250ec72",
   "metadata": {},
   "source": [
    "## Load in our query embedder as well as other helpful files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05817d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I don't think notebooks lets you just import a single function or class from another file, so we have to redefine\n",
    "#the model class in order to load our trained query embedder\n",
    "class TransformerEmbed(torch.nn.Module):\n",
    "    '''\n",
    "    TransformerEncoder for word embeddings. Since the synthetic queries are generated with out any type of order,\n",
    "    we don't use a positional encoding.\n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim, transformer_dim=512):\n",
    "        super(TransformerEmbed, self).__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=8,batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "        \n",
    "        self.initial = nn.Linear(in_dim,transformer_dim)\n",
    "        self.final = nn.Linear(transformer_dim, out_dim)\n",
    "        self.float()\n",
    "    def call(self, inputs):\n",
    "        x = self.initial(inputs)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "transformer = TransformerEmbed(100,80)\n",
    "transformer.load_state_dict(torch.load('./query_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117b5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('most_popular_title_in_order') as json_file:\n",
    "    most_popular_title_in_order = json.load(json_file)\n",
    "\n",
    "with open('dict.json') as json_file:\n",
    "    correct_word_dict = json.load(json_file)\n",
    "    \n",
    "with open('book_to_int.json') as json_file:\n",
    "    book_to_int = json.load(json_file)\n",
    "    \n",
    "with open('int_to_book.json') as json_file:\n",
    "    int_to_book = json.load(json_file)\n",
    "    \n",
    "with open('book_id_to_correct_title.json') as json_file:\n",
    "    book_id_to_correct_title = json.load(json_file)\n",
    "    \n",
    "with open('incorrect_title_to_book_id.json') as json_file:\n",
    "    incorrect_title_to_book_id = json.load(json_file)\n",
    "    \n",
    "with open('int_to_weight.json') as json_file:\n",
    "    temp = json.load(json_file)\n",
    "int_to_weight = {int(k):np.asarray(v) for k,v in temp.items()}\n",
    "\n",
    "corpus = gensim_api.load('text8')\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52252b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorWrapper():\n",
    "    '''\n",
    "    class which wraps operations in the vector space\n",
    "    so we never have to think about it in the main search class.\n",
    "    '''\n",
    "    def __init__(self,int_to_vector_dict, book_to_int, int_to_book):\n",
    "        self.int_to_vector_dict = int_to_vector_dict\n",
    "        self.book_to_int = book_to_int\n",
    "        self.int_to_book = int_to_book\n",
    "        \n",
    "        #We are defining cosine similarity as the distance metric. This is just the normed dot product.\n",
    "        self.scoring_fxn = nn.CosineSimilarity(dim=1, eps=1e-6) \n",
    "        self.vector_tensor = self.__get_vectors_as_tensors()\n",
    "        \n",
    "    def title_to_vector(self,title):\n",
    "        book_int = self.int_to_book[title]\n",
    "        \n",
    "    #function for combing vectors into a tensor for fast distance calculation\n",
    "    def __get_vectors_as_tensors(self):\n",
    "        vector_tensor = []\n",
    "        for i in range(len(self.book_to_int)):\n",
    "            vector_tensor.append(self.int_to_vector_dict[i+1])\n",
    "        \n",
    "        return torch.tensor(vector_tensor)     \n",
    "        \n",
    "    def __compute_distance(self,vector1,vector2):\n",
    "        if len(vector1.shape) == 1: \n",
    "            vector1 = torch.unsqueeze(vector1, dim = 0)\n",
    "        \n",
    "        if len(vector2.shape) == 1:\n",
    "            vector2 = torch.unsqueeze(vector2, dim = 0)\n",
    "        \n",
    "        return self.scoring_fxn(vector1,vector2)\n",
    "    \n",
    "    def compute_nearest_booksV(self,vector,num):\n",
    "        l = list(self.__compute_nearest_vectors(vector,num))\n",
    "        return [int_to_book[str(int(i)+1)] for i in l]\n",
    "        \n",
    "    def compute_nearest_booksT(self,title,num):\n",
    "        book_int = self.book_to_int[title]\n",
    "        vector = torch.tensor(self.int_to_vector_dict[book_int])\n",
    "        l = list(self.__compute_nearest_vectors(vector,num))\n",
    "        return [int_to_book[str(int(i)+1)] for i in l]\n",
    "        \n",
    "    # stack the vector we're concerned with so that we can compute distances in parallel. Yay pytorch!     \n",
    "    def __compute_nearest_vectors(self,vector,num):\n",
    "        if len(vector.shape) == 1:\n",
    "            vector = torch.unsqueeze(vector, dim = 0)\n",
    "            \n",
    "        __vector_stack = torch.cat([vector for _ in range(len(self.book_to_int))])\n",
    "        \n",
    "        scores = self.scoring_fxn(__vector_stack,self.vector_tensor)\n",
    "        return torch.topk(scores,num,sorted=True)[1]\n",
    "        \n",
    "    def compute_matches_from_vector(self,vector):\n",
    "        return self.__compute_nearest_vector(vector)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca209b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAlgo():\n",
    "    '''class for handling logic of searches'''\n",
    "    def __init__(self, book_vectors, w2v, query_model, fuzzy_cutoff, book_to_int):\n",
    "        self.past_searches = []\n",
    "        self.vector_wrapper = book_vectors\n",
    "        self.fuzzy_cutoff = fuzzy_cutoff      \n",
    "        self.titles = book_to_int.keys()\n",
    "        self.book_to_int = book_to_int\n",
    "        self.w2v = w2v\n",
    "        self.query_model = query_model\n",
    "        self.tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "        \n",
    "        \n",
    "    def get_nearest_books(self,vector,num):\n",
    "        return self.vector_wrapper.compute_nearest_booksV(vector,num)\n",
    "    \n",
    "    # This function handles queries which weren't matched with titles.\n",
    "    # It first calls __parse_query which deals with cleaning up the query. \n",
    "    # This intails filtering out stop words as well as words which the word2vec model doesn't know about\n",
    "    \n",
    "    def query_to_language_model(self, query, num):\n",
    "        query, success = self.__parse_query(query)\n",
    "        if not success:\n",
    "            return None, False\n",
    "        \n",
    "        \n",
    "        query = torch.tensor([[self.w2v.wv[q] for q in query]])\n",
    "        vector = self.query_model.call(query)\n",
    "        \n",
    "        books = self.get_nearest_books(vector, num)\n",
    "        return books, True\n",
    "        \n",
    "    def __parse_query(self, query):\n",
    "        query = self.tokenizer.tokenize(query)\n",
    "        query = [word.lower() for word in query if (word.lower() not in stopwords and word.lower() in self.w2v.wv.key_to_index.keys())]\n",
    "        \n",
    "        return(query,len(query)>0)\n",
    "    \n",
    "    # checks to see for matches on titles. \n",
    "    # This would need more engineering to make sure your not matching when you dont want to be.\n",
    "    def __check_titles(self,query):\n",
    "        print(query)\n",
    "        for title in most_popular_title_in_order:\n",
    "            if type(title) == str:\n",
    "                if query.lower() in title.lower():\n",
    "                    return title\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "            \n",
    "    # main method which mostly just handles the logic, nothing so special\n",
    "    def make_query(self, query, num = 5):\n",
    "        #no searching by author!\n",
    "        title_match = self.__check_titles(query)\n",
    "        if title_match:\n",
    "            print('title found')\n",
    "            return self.vector_wrapper.compute_nearest_booksT(title_match, num)\n",
    "            \n",
    "        else:\n",
    "            books,success = self.query_to_language_model(query, num)\n",
    "            if not success:\n",
    "                print(\"You entered a confusing word! No good recomendations...\")\n",
    "                return None\n",
    "            return books\n",
    "            \n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43335022",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorWrapper = VectorWrapper(int_to_weight,book_to_int,int_to_book)\n",
    "search = SearchAlgo(vectorWrapper,model,transformer, .8, book_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7811dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry potter\n",
      "title found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Harry Potter and the Philosopher's Stone\",\n",
       " 'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Prisoner of Azkaban',\n",
       " 'Harry Potter and the Goblet of Fire',\n",
       " 'Harry Potter and the Half-Blood Prince',\n",
       " 'Harry Potter and the Deathly Hallows',\n",
       " 'Frostbite',\n",
       " 'Shadow Kiss',\n",
       " 'Blood Promise',\n",
       " 'Complete Harry Potter Boxed Set']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.make_query('harry potter',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94598d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
